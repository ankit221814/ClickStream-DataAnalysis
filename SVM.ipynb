{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1iQ/D5Kn0BciGWbfRAVLb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankit221814/ClickStream-DataAnalysis/blob/main/SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Answwer:- Information Gain is a measure used to decide which feature should be selected to split the data at each node in a decision tree. It is based on the concept of Entropy, which measures the impurity or randomness in the dataset.\n",
        "Formula:\n",
        "Information Gain\n",
        "=\n",
        "Entropy(parent)\n",
        "−\n",
        "∑\n",
        "i\n",
        "(\n",
        "∣\n",
        "D\n",
        "i\n",
        "∣\n",
        "∣\n",
        "D\n",
        "∣\n",
        "×\n",
        "Entropy\n",
        "(\n",
        "D\n",
        "i\n",
        ")\n",
        ")\n",
        "Information Gain=Entropy(parent)−\n",
        "i\n",
        "∑\n",
        "​\n",
        " (\n",
        "∣D∣\n",
        "∣D\n",
        "i\n",
        "​\n",
        " ∣\n",
        "​\n",
        " ×Entropy(D\n",
        "i\n",
        "​\n",
        " ))\n",
        "Usage:\n",
        "At each node, the feature that gives maximum Information Gain is chosen for splitting."
      ],
      "metadata": {
        "id": "KBhIKyyXj6cB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Difference between Gini Impurity and Entropy.\n",
        "\n",
        "Answer:\n",
        "Feature\tGini Impurity\tEntropy\n",
        "Formula\n",
        "1\n",
        "−\n",
        "∑\n",
        "p\n",
        "i\n",
        "2\n",
        "1−∑p\n",
        "i\n",
        "2\n",
        "​\n",
        "\n",
        "−\n",
        "∑\n",
        "p\n",
        "i\n",
        "log\n",
        "⁡\n",
        "2\n",
        "p\n",
        "i\n",
        "−∑p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        "\n",
        "Computation\tFaster\tSlower\n",
        "Accuracy\tSlightly less precise\tMore precise\n",
        "Usage\tDefault in CART trees\tUsed in ID3, C4.5\n",
        "Best for\tLarge datasets\tSmaller datasets\n",
        "Conclusion:\n",
        "Gini → Faster\n",
        "Entropy → More informative"
      ],
      "metadata": {
        "id": "2N5MzJtwj6Zb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "Pre-pruning is a technique to stop the growth of a decision tree early to prevent overfitting.\n",
        "Common Pre-pruning methods:\n",
        "Setting max_depth\n",
        "Setting min_samples_split\n",
        "Setting min_samples_leaf\n",
        "Benefits:\n",
        "Reduces overfitting\n",
        "Improves generalization\n",
        "Speeds up training\n"
      ],
      "metadata": {
        "id": "RNMz06-2j6W0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "#Impurity as the criterion and print the feature importances (practical).\n",
        "#Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "#(Include your Python code and output in the code box below.)\n",
        "\n",
        "#Answer:-\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train model\n",
        "model = DecisionTreeClassifier(criterion='gini')\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print feature importance\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRKSjcwEk0Hg",
        "outputId": "c57970e1-f6e5-4c8a-90c1-4a15a777057f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances: [0.01333333 0.         0.56405596 0.42261071]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "Answer:\n",
        "SVM is a supervised machine learning algorithm used for classification and regression. It finds the optimal hyperplane that maximizes the margin between classes.\n",
        "Key Concepts:\n",
        "Support Vectors\n",
        "Margin maximization\n",
        "Kernel functions"
      ],
      "metadata": {
        "id": "TBo8DaT8j6RW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the Kernel Trick in SVM?\n",
        "Answer:\n",
        "The Kernel Trick allows SVM to transform data into higher dimensions to make non-linear data separable without explicitly computing the transformation.\n",
        "Common Kernels:\n",
        "Linear\n",
        "Polynomial\n",
        "RBF (Gaussian)\n",
        "Sigmoid"
      ],
      "metadata": {
        "id": "_loVBPq2j6OT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "#kernels on the Wine dataset, then compare their accuracies.\n",
        "#Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "#on the same dataset.\n",
        "#(Include your Python code and output in the code box below.)\n",
        "\n",
        "#Answer:-\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Linear Kernel\n",
        "linear_model = SVC(kernel='linear')\n",
        "linear_model.fit(X_train, y_train)\n",
        "linear_pred = linear_model.predict(X_test)\n",
        "linear_acc = accuracy_score(y_test, linear_pred)\n",
        "\n",
        "# RBF Kernel\n",
        "rbf_model = SVC(kernel='rbf')\n",
        "rbf_model.fit(X_train, y_train)\n",
        "rbf_pred = rbf_model.predict(X_test)\n",
        "rbf_acc = accuracy_score(y_test, rbf_pred)\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", linear_acc)\n",
        "print(\"RBF Kernel Accuracy:\", rbf_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9jJzNx1lP5j",
        "outputId": "55bc1d06-d80b-4334-cf0e-3c02b8c7092f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.9814814814814815\n",
            "RBF Kernel Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "Answer:\n",
        "Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem.\n",
        "It is called \"Naïve\" because it assumes all features are independent, which is usually not true in real-world data."
      ],
      "metadata": {
        "id": "wX95GbNbleFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Differences between Gaussian, Multinomial & Bernoulli Naïve Bayes\n",
        "Answer:\n",
        "Type\tUsed For\tData Type\n",
        "Gaussian NB\tContinuous data\tReal values\n",
        "Multinomial NB\tText classification\tCount-based data\n",
        "Bernoulli NB\tBinary features\tYes/No values"
      ],
      "metadata": {
        "id": "8H2d1lczlgMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 10: Breast Cancer Dataset\n",
        "#Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "#dataset and evaluate accuracy.\n",
        "#Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from sklearn.datasets. (Include your Python code and output in the code box below.)\n",
        "\n",
        "#Answer:-\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vzu0ZziQlpPv",
        "outputId": "9cd6825e-a596-4207-f02f-9ffea762709b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9415204678362573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WzSsWcdml4ey"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}